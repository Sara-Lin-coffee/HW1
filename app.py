import time
import numpy as np
import streamlit as st
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

st.set_page_config(page_title="AutoInit Linear Regression (CRISP-DM)", page_icon="ğŸ“ˆ", layout="wide")

# ---------- Sidebar: CRISP-DM ----------
with st.sidebar:
    st.title("CRISP-DM å°è¦½")
    steps = [
        ("Business Understanding", "ç”¨ç·šæ€§å›æ­¸æ“¬åˆ y ~ x"),
        ("Data Understanding", "æª¢è¦–è³‡æ–™åˆ†ä½ˆèˆ‡å™ªè²"),
        ("Data Preparation", "åˆ‡åˆ† train/test"),
        ("Modeling", "äº‚æ•¸åˆå§‹å€¼ + æ¢¯åº¦ä¸‹é™ï¼ˆå«ç©©å®šåŒ–ï¼‰/ sklearn å°ç…§"),
        ("Evaluation", "RÂ²ã€RMSEã€MAEã€Loss Curve"),
        ("Deployment", "æœ¬æ©Ÿ & Streamlit Cloud")
    ]
    for name, desc in steps:
        st.markdown(f"- **{name}** â€” {desc}")

# ---------- Header ----------
st.title("ğŸ“ˆ ç·šæ€§å›æ­¸ï¼ˆè‡ªå‹•äº‚æ•¸åˆå§‹å€¼ + è¦–è¦ºåŒ–ï¼‰â€” sklearn + CRISP-DM")
st.caption("å…§å»ºæ•¸å€¼ç©©å®šåŒ–ï¼šç‰¹å¾µæ¨™æº–åŒ–ã€å­¸ç¿’ç‡è‡ªå‹•å›é€€ï¼ˆloss ä¸Šå‡æ™‚ï¼‰ï¼Œé¿å… NaN/Infã€‚")

# ---------- Controls ----------
colA, colB, colC, colD = st.columns([1.2, 1.2, 1.2, 1.4], vertical_alignment="bottom")
with colA:
    n_samples = st.slider("æ¨£æœ¬æ•¸", 30, 1000, 200, step=10)
with colB:
    noise = st.slider("å™ªè²æ¨™æº–å·®", 0.0, 10.0, 2.5, 0.1)
with colC:
    lr_init = st.slider("åˆå§‹å­¸ç¿’ç‡ (GD)", 1e-5, 1e-1, 1e-2, format="%.5f")
with colD:
    n_iters = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•¸", 10, 3000, 500, step=10)

colE, colF, colG = st.columns([1.2, 1.2, 1.6], vertical_alignment="bottom")
with colE:
    seed_data = st.number_input("è³‡æ–™äº‚æ•¸ç¨®å­", min_value=0, value=int(time.time()) % 10**6, step=1)
with colF:
    seed_init = st.number_input("åƒæ•¸åˆå§‹äº‚æ•¸ç¨®å­", min_value=0, value=(int(time.time()*1.7) % 10**6), step=1)
with colG:
    use_standardize = st.checkbox("ä½¿ç”¨ç‰¹å¾µæ¨™æº–åŒ–ï¼ˆå»ºè­°ï¼‰", value=True, help="å° Xã€y åšæ¨™æº–åŒ–å†ç”¨ GDï¼Œæœ€å¾Œå†è½‰å›åŸå§‹åº§æ¨™ä»¥ä¾¿ç¹ªåœ–/è©•ä¼°ã€‚")

# ---------- Data Generation ----------
rng_data = np.random.default_rng(seed_data)
true_w = rng_data.normal(loc=3.0, scale=1.0)     # çœŸå¯¦æ–œç‡
true_b = rng_data.normal(loc=1.0, scale=2.0)     # çœŸå¯¦æˆªè·
X = rng_data.uniform(-10, 10, size=(n_samples, 1))
y = true_w * X[:, 0] + true_b + rng_data.normal(0, noise, size=n_samples)

# ---------- Split ----------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---------- (å¯é¸) æ¨™æº–åŒ–ï¼šç©©å®š GD ----------
def standardize(train_arr, test_arr):
    mean = np.mean(train_arr)
    std = np.std(train_arr)
    std = std if std > 1e-12 else 1.0  # é¿å…é™¤ä»¥ 0
    return (train_arr - mean) / std, (test_arr - mean) / std, mean, std

if use_standardize:
    Xtr_s, Xte_s, mx, sx = standardize(X_train[:, 0], X_test[:, 0])
    ytr_s, yte_s, my, sy = standardize(y_train, y_test)
    Xtr_s = Xtr_s.reshape(-1, 1)
    Xte_s = Xte_s.reshape(-1, 1)
else:
    # ä¿æŒåŸå°ºåº¦
    Xtr_s, Xte_s = X_train.copy(), X_test.copy()
    ytr_s, yte_s = y_train.copy(), y_test.copy()
    mx = np.mean(X_train[:, 0]); sx = np.std(X_train[:, 0]); sx = sx if sx > 1e-12 else 1.0
    my = np.mean(y_train);       sy = np.std(y_train);       sy = sy if sy > 1e-12 else 1.0

# ---------- Random Init for Gradient Descent ----------
rng_init = np.random.default_rng(seed_init)
w_s = rng_init.normal(loc=0.0, scale=1.0)  # åœ¨æ¨™æº–åŒ–ç©ºé–“åˆå§‹åŒ–
b_s = rng_init.normal(loc=0.0, scale=1.0)

# ---------- Gradient Descent with safety ----------
def finite(x): 
    return np.all(np.isfinite(x))

loss_hist = []
wb_hist = []
lr = lr_init
best_loss = np.inf
best_wb = (w_s, b_s)
patience = 30  # å®¹è¨±è‹¥å¹²æ¬¡ä¸Šå‡å¾Œä»æŒçºŒå˜—è©¦
bad_steps = 0

for t in range(n_iters):
    # é æ¸¬ (åœ¨æ¨™æº–åŒ–ç©ºé–“)
    y_pred_s = w_s * Xtr_s[:, 0] + b_s
    err = y_pred_s - ytr_s
    loss = float(np.mean(err ** 2))

    # NaN/Inf ç«‹å³è™•ç†
    if not finite([loss]):
        bad_steps += 1
        lr *= 0.5
        if lr < 1e-7:
            st.warning("åµæ¸¬åˆ°æ•¸å€¼ä¸ç©©å®šï¼Œå·²æå‰åœæ­¢ã€‚å»ºè­°é™ä½å­¸ç¿’ç‡æˆ–å‹¾é¸æ¨™æº–åŒ–ã€‚")
            break
        continue

    loss_hist.append(loss)
    wb_hist.append((w_s, b_s))

    # ç´€éŒ„æœ€ä½³
    if loss < best_loss:
        best_loss = loss
        best_wb = (w_s, b_s)
        bad_steps = 0
    else:
        bad_steps += 1
        # è‹¥ loss ä¸Šå‡ï¼Œå›é€€ä¸€æ­¥ä¸¦é™ä½ lr
        w_s, b_s = best_wb
        lr *= 0.5

    # æ¢¯åº¦
    dw = 2.0 * np.mean(err * Xtr_s[:, 0])
    db = 2.0 * np.mean(err)

    # æ›´æ–°å‰æª¢æŸ¥
    w_new = w_s - lr * dw
    b_new = b_s - lr * db

    if not finite([w_new, b_new]):
        lr *= 0.5
        if lr < 1e-7:
            st.warning("åµæ¸¬åˆ°æ•¸å€¼ä¸ç©©å®šï¼Œå·²æå‰åœæ­¢ã€‚å»ºè­°é™ä½å­¸ç¿’ç‡æˆ–å‹¾é¸æ¨™æº–åŒ–ã€‚")
            break
        continue

    w_s, b_s = w_new, b_new

    # è‹¥ä¸æ–·è®Šå·®ä¹Ÿåœæ­¢
    if bad_steps > patience:
        st.info("å¤šæ¬¡ loss ä¸Šå‡ï¼Œå·²æå‰åœæ­¢ä¸¦æ¡ç”¨æœ€ä½³åƒæ•¸ã€‚å¯é™ä½å­¸ç¿’ç‡æˆ–å¢åŠ æ¨™æº–åŒ–ã€‚")
        break

# ä½¿ç”¨æœ€ä½³åƒæ•¸
w_s, b_s = best_wb

# ---------- è½‰å›åŸå§‹ç©ºé–“ä¿‚æ•¸ï¼ˆä¾¿æ–¼ç¹ªåœ–èˆ‡èˆ‡ sklearn å°ç…§ï¼‰ ----------
# æ¨™æº–åŒ–å®šç¾©ï¼šXs = (x - mx) / sxï¼Œys = (y - my) / sy
# æ–¼æ¨™æº–åŒ–ç©ºé–“ï¼šys â‰ˆ w_s * Xs + b_s
# => y â‰ˆ sy * (w_s * (x - mx)/sx + b_s) + my
# => æ–œç‡ w = sy * w_s / sx
#    æˆªè· b = sy * (b_s - w_s * mx / sx) + my
gd_w = float(sy * w_s / sx)
gd_b = float(sy * (b_s - w_s * mx / sx) + my)

# ---------- sklearn LinearRegression å°ç…§ ----------
sk_model = LinearRegression(fit_intercept=True)
sk_model.fit(X_train, y_train)
sk_w = float(sk_model.coef_[0])
sk_b = float(sk_model.intercept_)

# ---------- Evaluation ----------
def eval_metrics(w_, b_, X_, y_):
    pred = w_ * X_[:, 0] + b_
    # å®‰å…¨ä¿è­·ï¼šè‹¥å‡ºç¾éæœ‰é™æ•¸ï¼Œå›å‚³ NaN-friendly çµæœ
    if not np.all(np.isfinite(pred)):
        return np.nan, np.nan, np.nan
    r2 = r2_score(y_, pred)
    rmse = float(np.sqrt(mean_squared_error(y_, pred)))
    mae = float(mean_absolute_error(y_, pred))
    return r2, rmse, mae

r2_train_gd, rmse_train_gd, mae_train_gd = eval_metrics(gd_w, gd_b, X_train, y_train)
r2_test_gd,  rmse_test_gd,  mae_test_gd  = eval_metrics(gd_w, gd_b, X_test,  y_test)
r2_train_sk, rmse_train_sk, mae_train_sk = eval_metrics(sk_w, sk_b, X_train, y_train)
r2_test_sk,  rmse_test_sk,  mae_test_sk  = eval_metrics(sk_w, sk_b, X_test,  y_test)

# ---------- Plots ----------
left, right = st.columns([1.1, 1])

with left:
    st.subheader("è³‡æ–™æ•£é»åœ–èˆ‡æ¨¡å‹æ“¬åˆ")
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots(figsize=(6.8, 4.8))
    ax.scatter(X[:, 0], y, alpha=0.45, label="è³‡æ–™é»")
    xs = np.linspace(X.min(), X.max(), 200)
    ax.plot(xs, true_w * xs + true_b, linestyle="--", label="çœŸå¯¦å‡½æ•¸")
    ax.plot(xs, gd_w * xs + gd_b, label="GD æ“¬åˆç·šï¼ˆäº‚æ•¸åˆå§‹+ç©©å®šåŒ–ï¼‰")
    ax.plot(xs, sk_w * xs + sk_b, label="sklearn æ“¬åˆç·š")
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.legend()
    st.pyplot(fig, use_container_width=True)

    st.subheader("Loss Curveï¼ˆMSEï¼‰")
    fig2, ax2 = plt.subplots(figsize=(6.8, 3.2))
    if len(loss_hist) > 0 and np.all(np.isfinite(loss_hist)):
        ax2.plot(loss_hist)
    else:
        ax2.plot([])
        ax2.text(0.5, 0.5, "ï¼ˆå·²å•Ÿå‹•æ•¸å€¼ä¿è­·ï¼Œæœªè¨˜éŒ„æœ‰æ•ˆ Lossï¼‰", ha="center", va="center")
    ax2.set_xlabel("è¿­ä»£æ¬¡æ•¸")
    ax2.set_ylabel("MSE")
    st.pyplot(fig2, use_container_width=True)

with right:
    st.subheader("åƒæ•¸èˆ‡è©•ä¼°")
    m1, m2 = st.columns(2)
    with m1:
        st.metric("GD æ–œç‡ w", f"{gd_w:.6f}")
        st.metric("GD æˆªè· b", f"{gd_b:.6f}")
        st.caption("ï¼ˆæ¨™æº–åŒ– + è‡ªå‹•å›é€€å­¸ç¿’ç‡ + å®‰å…¨é˜²è­·ï¼‰")
    with m2:
        st.metric("sklearn æ–œç‡ w", f"{sk_w:.6f}")
        st.metric("sklearn æˆªè· b", f"{sk_b:.6f}")

    st.markdown("### é©—è­‰é›†ï¼ˆTestï¼‰è¡¨ç¾")
    def fmt(x): 
        return "NaN" if (x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x)))) else f"{x:.6f}"
    st.write(
        f"- **GD**ï¼šRÂ² = {fmt(r2_test_gd)}ï½œRMSE = {fmt(rmse_test_gd)}ï½œMAE = {fmt(mae_test_gd)}\n"
        f"- **sklearn**ï¼šRÂ² = {fmt(r2_test_sk)}ï½œRMSE = {fmt(rmse_test_sk)}ï½œMAE = {fmt(mae_test_sk)}"
    )

st.divider()
st.markdown(
    "### CRISP-DM å°çµ\n"
    "- **Business Understanding**ï¼šä»¥ç·šæ€§å›æ­¸é æ¸¬ y ~ xã€‚\n"
    "- **Data Understanding**ï¼šå¯è¦–åŒ–æ•£é»èˆ‡å™ªè²ã€‚\n"
    "- **Data Preparation**ï¼štrain/test splitï¼ˆ80/20ï¼‰ï¼Œï¼ˆå¯é¸ï¼‰æ¨™æº–åŒ–ã€‚\n"
    "- **Modeling**ï¼šäº‚æ•¸åˆå§‹ + æ¢¯åº¦ä¸‹é™ï¼Œå«å­¸ç¿’ç‡å›é€€èˆ‡æ•¸å€¼ä¿è­·ï¼›å°ç…§ sklearnã€‚\n"
    "- **Evaluation**ï¼šRÂ² / RMSE / MAE èˆ‡ Loss Curveã€‚\n"
    "- **Deployment**ï¼šæœ¬æ©Ÿ `streamlit run app.py`ï¼Œæˆ–éƒ¨ç½²åˆ° Streamlit Cloudã€‚"
)

